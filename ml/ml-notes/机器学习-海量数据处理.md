[toc]

# 一、为什么需要海量数据？

我们已经知道一种获取高性能的机器学习系统的途径是采用低偏差的学习算法，并用大数据进行训练。**即决定效果好坏的往往不是算法的好坏，而是谁的训练数据多。**如果你想使用大数据进行训练，至少要能获得大数据集。

![img](https://tva1.sinaimg.cn/large/0082zybply1gbz8qrs60oj30s30f3q91.jpg)



# 二、高效的大数据处理算法

如果有一亿条训练数据，可以和从这一亿条数据中随机抽取一千条数据训练的结果相同，为什么我们不用一千条进行训练呢？增大训练数据是否能提升算法的好坏？我们可以看下图，当训练数据增大到一定数量的时候，训练的误差是趋于稳定的，也就是说并不需要这么多的数据量。

![img](https://tva1.sinaimg.cn/large/0082zybply1gbz8v2eztdj30ts0g1dn5.jpg)

下面介绍几种算法来高效的处理大数据：

>+ **随机梯度下降法**
>+  **Mini-Batch梯度下降**

## 2.1 **随机梯度下降法**

在介绍这个算法之前，我们先来看看普通的批量梯度下降算法。对于很多机器学习算法，例如线性回归、逻辑回归和神经网络，我们推导算法的方法是提出一个代价函数，或提出一个优化目标，然后使用梯度下降的算法求代价函数的最小值。例如逻辑回归中，我们的算法如下

<img src="https://tva1.sinaimg.cn/large/0082zybply1gbz90ykeivj30su0edagu.jpg" alt="img" style="zoom:67%;" />

当训练数据量非常大的时候，可以看到每次进行梯度计算的时候都需要考虑全部的样本总数 $m$ ，需要不断的迭代，不断的求和，这个过程非常的耗时，那么怎么解决这个问题呢？**随机梯度法**(Stohastic gradient descent)应运而生，其算法如下：

> 1. 随机打乱所有数据
>
> 2. Repeat(1-10)
>
>     1. for $i \to m$
>         $$
>         \theta_{j}:=\theta_{j}-\alpha\left(h_{\theta}\left(x^{(i)}\right)-y^{(i)}\right) x_{j}^{(i)} \quad j=0,1,\ldots,n \tag{1}
>         $$

可以看到，上述算法每次迭代都只需要计算一个样本的梯度，这样大大提高了计算的效率，但是这样计算最终会收敛吗？

> **和批量梯度下降相比，收敛方式是不同的，随机梯度下降法就是连续不断地在某个区域内朝着全局最小值徘徊，而不是直接达到全局最小值。但通常我们使用随机梯度下降就可以达到一个很好的全局最小值，这对于算法的实际效果就已经足够了。**

最后，在随机梯度下降法中有一个外层循环，它决定了内层循环的执行次数，所以外层循环应该执行多少次呢？这取决于训练集的大小，通常一次就够了（1-10次）。

## 2.2 Mini-Batch梯度下降

Mini-Batch梯度下降比随机梯度下降还要快！它的思想是既不使用全部的样本，也不一次只用一个样本，而是一次使用 $b$ 个样本。其算法如下

> 假设 $b = 10, m=1000$
>
> 1. Repeat(1-10)
>
>     1. for $i =1,11,21,\ldots,991$
>         $$
>         \theta_{j}:=\theta_{j}-\alpha\frac{1}{10}\sum_{k=i}^{i+9}\left(h_{\theta}\left(x^{(k)}\right)-y^{(k)}\right) x_{j}^{(k)} \quad j=0,1,\ldots,n \tag{2}
>         $$

Mini-Batch梯度下降的好处在于每次运行了 $b$ 个样本，这比随机梯度更快更容易收敛。但缺点也是很明显的：需要额外的确定参数 $b$ 。

# 三、在线学习

在线学习机制让我们可以模型化一些问题，就是我们有连续一波数据或者连续的数据流，先要用算法从中学习的这类问题。网站用户数据流，就可以用在线学习机制从数据流中学习用户的偏好，然后用这些信息优化关于网站的决策。

在在线学习机制中，我们实际上丢弃了**固定的数据集**这一概念，取而代之是一个算法，现在我们获取一个样本，然后利用那个样本以这种方式学习。然后我们丢弃这个样本。因为你有强大的数据流，没必要反复使用样本。

这种在线学习机制可以适应变化的用户偏好，例如

> 电商网站的购物推荐
>
> 新闻内容推荐
>
> ……

# 四、减少映射与数据并行

这种方法主要应用在大规模机器学习上，可以将学习算法应用到随机梯度下降不能解决的规模更大的问题。

把训练集分成10份或者100份，计算完成后再返回到一个服务器上。这个公式完全等同于批量梯度下降算法，只是不需要在一台机器上计算。

举个栗子，将原始数据量为 400 的数据集等分成 4 份到 4 台不同的机器处理，最后将 4 台机器的结果进行汇总。

![image-20200217172744283](https://tva1.sinaimg.cn/large/0082zybply1gbzizava6dj31k60u0kjm.jpg)

关于分布式编程方法，可以移步到我的另外一个专栏👉[[动手学pySpark]]()。Spark 基于 MapReduce 技术来进行分布式计算，只要学习算法可以表示成一系列的求和形式，或者表示成在训练集上对函数的求和形式，你就可以使用MapReduce技巧来并行化学习算法，使得其可以应用于非常大的数据集。

计算机可以有多个CPU，CPU又有多个核心，如果你有很大的数据集，且你有一个四核电脑，可以用MapReduce分共工作，然后每个核心计算四分之一训练样本的总和。还不用担心数据传输和网络延迟。有些线性代数库可以自动在一台机器上不同核心上进行并行代数运算。而且如果你的学习算法有非常好的向量化表示，你就可以直接以向量化的形式应用标准学习算法，不用担心并行。你的线性代数库会帮你处理的很好，所以你可以不应用MapReduce。

![image-20200217173357848](https://tva1.sinaimg.cn/large/0082zybpgy1gbzj5s2volj30xe0hytev.jpg)

# 五、参考

[1] https://www.jianshu.com/p/bd861b5b05d8

[2] Andrew Ng.机器学习视频