

前几天用到一个用于获取多维特征数据的权重方法——熵权法，就想着整理一下信息论中的熵的相关概念，毕竟有些证明时间久了就给忘了。

![](https://tva1.sinaimg.cn/large/007S8ZIlly1gewyzm76d1j317u0nawhd.jpg)

**克劳德·艾尔伍德·香农**（英语：**Claude Elwood Shannon**，1916年4月30日－2001年2月26日），[美国](https://zh.wikipedia.org/wiki/美国)[数学家](https://zh.wikipedia.org/wiki/数学家)、[电子工程师](https://zh.wikipedia.org/w/index.php?title=电子工程师&action=edit&redlink=1)和[密码学家](https://zh.wikipedia.org/w/index.php?title=密码学家&action=edit&redlink=1)，被誉为[信息论](https://zh.wikipedia.org/wiki/信息论)的创始人。香农是[密歇根大学](https://zh.wikipedia.org/wiki/密西根大學)学士，[麻省理工学院](https://zh.wikipedia.org/wiki/麻省理工學院)博士。1948年，香农发表了划时代的论文——[通信的数学原理](https://zh.wikipedia.org/w/index.php?title=通信的数学原理&action=edit&redlink=1)，奠定了现代信息论的基础。

## 什么是熵

Shannon提出了熵的概念。**熵是一个随机变量不确定性的度量**，对于一个离散型随机变量 $X \text{~} p(x)$，离散熵可以定义为
$$
H(x)=-\sum_{x \in X}p(x)\log p(x)\tag{1}
$$
其中，$\log $ 一般以 2 为底。为了加深对熵的理解，下面举两个例子来说明。

> 【例 1】设随机变量 $X$ 为抛一枚均匀硬币的取值，其中正面朝上用 1 表示，反面朝上用 0 表示 ，求解随机变量 $X$ 的熵。

【解】由于
$$
P(X=0)=P(X=1)=\frac{1}{2}
$$
所以
$$
H(x)=-\frac{1}{2}\log \frac{1}{2} -\frac{1}{2}\log \frac{1}{2} =1
$$

> 【例 2】设随机变量 $Y$ 为抛一个六面均匀的筛子，其中 $Y=\{1,2,3,4,5,6\}$ ，求解随机变量 $Y$ 的熵。

【解】由于
$$
P(Y=i)=\frac{1}{6},i \in \{1,2,3,4,5,6\}
$$
所以
$$
H(y)=-\frac{1}{6}\log \frac{1}{2} ×6 = \log 6
$$
由于 $1=\log 2 < \log 6$， 所以随机变量 $X$ 的不确定性比 $Y$ 小。



> 👉推论：必然事件的熵为 0 。



## 熵的性质

信息论之父克劳德·香农给出的信息熵的三个性质

1. **单调性**，发生概率越高的事件，其携带的信息量越低；
2. **非负性**，信息熵可以看作为一种广度量，非负性是一种合理的必然；
3. **累加性**，即多随机事件同时发生存在的总不确定性的量度是可以表示为各事件不确定性的量度的和，这也是广度量的一种体现。



##拓展概念

### 条件熵

条件熵 $H(Y|X)$ 表示在已知随机变量 $X$ 的条件下随机变量 $Y$  的不确定性。
$$
\begin{aligned}
H(Y | X) &=\sum_{x} p(x) H(Y | X=x) \\
&=-\sum_{x} p(x) \sum_{y} p(y | x) \log p(y | x) \\
&=-\sum_{x} \sum_{y} p(x, y) \log p(y | x) \\
&=-\sum_{x, y} p(x, y) \log p(y | x)
\end{aligned}\tag{2}
$$

### 联合熵

联合熵表征了两事件同时发生系统的不确定度，设分布为 $p(x,y)$ 的一对随机变量 $(X,Y)$  ，其联合熵定义为
$$
H(X, Y)=-\sum_{x \in X} \sum_{y \in Y} p(x, y) \log p(x, y)\tag{3}
$$
其数学推导如下
$$
\begin{aligned}
H(X, Y) &=-\sum_{x \in X} \sum_{y \in Y} p(x, y) \log p(x, y) \\
&=-\sum_{x \in X} \sum_{y \in Y} p(x, y) \log p(x) p(y | x) \\
&=-\sum_{x \in X} \sum_{y \in Y} p(x, y) \log p(x)-\sum_{x \in X} \sum_{y \in Y} p(x, y) \log p(y | x) \\
&=-\sum_{x \in X} p(x) \log p(x)-\sum_{x \in X} \sum_{y \in Y} p(x, y) \log p(y | x) \\
&=H(X)+H(Y | X)
\end{aligned} \tag{4}
$$
由上式可以知道
$$
H(X, Y)=H(Y)+H(X | Y)\tag{5}
$$
由上面两个式子可以得到先观察哪个随机变量对最终信息量的获取是没有影响的。基于这个结论，我们可以推导如果有 $n$ 个随机变量处于一个随机系统中，那么我们获取其联合熵也是无关观察先后吗？答案是肯定的。为了说明原因，我们给出熵的链式法则

设随机变量 $X_1,X_2,\ldots,X_n$ 服从 $p(x_1,x_2,\ldots,x_n)$，则
$$
H\left(X_{1}, X_{2}, \cdots, X_{n}\right)=\sum_{i=1}^{n} H\left(X_{i} | X_{i-1}, \cdots, X_{1}\right) \tag{6}
$$
上式的证明
$$
\begin{aligned}
H\left(X_{1}, X_{2}, \cdots, X_{n}\right)&=-\sum_{x_{1}, \cdots, x_{n} \in N^{n}} p\left(x_{1}, \cdots, x_{n}\right) \log p\left(x_{1}, \cdots, x_{n}\right) \\

&=-\sum_{x_{1}, \cdots, x_{n} \in X^{n}} p\left(x_{1}, \cdots, x_{n}\right) \log p\left(x_{1}, \cdots, x_{n-1}\right) p\left(x_{n} | x_{1}, \cdots, x_{n-1}\right) \\

&=-\sum_{x_{1}, \cdots, x_{n} \in X^{n}} p\left(x_{1}, \cdots, x_{n}\right) \log p\left(x_{1}, \cdots, x_{n-2}\right) p\left(x_{n-1} | x_{1}, \cdots, x_{n-2}\right) p\left(x_{n} | x_{1}, \cdots, x_{n-1}\right) \\

&=-\sum_{x_{1}, \cdots, x_{n} \in X^{n}} p\left(x_{1}, \cdots, x_{n}\right) \log \prod_{i=1}^{n} p\left(x_{i} | x_{i-1}, \cdots, x_{1}\right)  \\

&=-\sum_{x_{1}, \cdots, x_{n} \in X^{n}} p\left(x_{1}, \cdots, x_{n}\right) \sum_{i=1}^{n} \log p\left(x_{i} | x_{i-1}, \cdots, x_{1}\right) \\

&=-\sum_{i=1}^{n} \sum_{x_{1}, \cdots, x_{i} \in X^{i}} p\left(x_{1}, \cdots, x_{i}\right) \log p\left(x_{i} | x_{i-1}, \cdots, x_{1}\right)=\sum_{i=1}^{n} H\left(X_{i} | X_{i-1}, \cdots, X_{1}\right)
\end{aligned}\tag{7}
$$
从链式法则，我们可以更进一步得到，如果随机变量 $X_1,X_2,\ldots,X_n$ 是独立的，那么联合熵则可以表示为
$$
H\left(X_{1}, X_{2}, \cdots, X_{n}\right)=\sum_{i=1}^{n} H\left(X_{i}\right)\tag{8}
$$



### 互信息

对于两个随机变量  $X$ 和 $Y$ ，如果其联合分布为 $p(x,y)$ ,边缘分布为 $p(x),p(y)$ ,则互信息可以定义为
$$
I(X ; Y)=\sum_{x \in X} \sum_{y \in Y} p(x, y) \log \frac{p(x, y)}{p(x) p(y)}\tag{9}
$$
我们做一下形式上的变换
$$
\begin{aligned}
I(X ; Y)&=\sum_{x \in X} \sum_{y \in Y} p(x, y) \log \frac{p(x, y)}{p(x) p(y)}\\
&=\sum_{x \in X} \sum_{y \in Y} p(x, y) \log \frac{p(y) p(x | y)}{p(x) p(y)}\\
&=\sum_{x \in X} \sum_{y \in Y} p(x, y) \log \frac{p(x | y)}{p(x)}\\
&=\sum_{x \in X} \sum_{y \in Y} p(x, y) \log p(x | y)-\sum_{x \in X} \sum_{y \in Y} p(x, y) \log p(x) \\
&=-\sum_{x \in X} p(x) \log p(x)-\left[-\sum_{x \in X} \sum_{y \in Y} p(x, y) \log p(x | y)\right]\\
&=H(X)-H(X | Y) \\
&=H(Y)-H(Y | X) \\
&=H(X,Y)-H(X|Y)-H(Y|X)
\end{aligned}\tag{10}
$$


上述变量的关系，可以使用 Venn 图表示如下

![image-20200518105445964](https://tva1.sinaimg.cn/large/007S8ZIlly1gewf0hu9n8j30p9092aal.jpg)



### 交叉熵

设 $p(x),q(x)$ 分别是离散随机变量 $X$ 的两个概率分布，其中 $p(x)$ 是目标分布，$p$ 和 $q$ 的交叉熵可以看做是使用分布 $ q(x)$  表示目标分布 $p(x)$ 的困难程度，可以表示为
$$
H(p, q)=\sum_{x \in X} p\left(x\right) \log \frac{1}{\log q\left(x\right)}=-\sum_{ x \in X} p\left(x\right) \log q\left(x\right)\tag{11}
$$

> 例【3】举个例子，考虑一个随机变量 $X$，其真实分布 $p(x)=(\frac{1}{2},\frac{1}{4},\frac{1}{8},\frac{1}{8})$ ，非真实分布为 $q(x)==(\frac{1}{4},\frac{1}{4},\frac{1}{4},\frac{1}{4})$，求解随机变量 $X$ 的交叉熵。

$$
\begin{aligned}
H(p, q) &=-\frac{1}{2} \log _{2} \frac{1}{4}-\frac{1}{4} \log _{2} \frac{1}{4}-\frac{1}{8} \log _{2} \frac{1}{4}-\frac{1}{8} \log _{2} \frac{1}{4} \\
&=1+\frac{1}{2}+\frac{1}{4}+\frac{1}{4} \\
&=2
\end{aligned}
$$

### 相对熵

也叫 **KL 散度**，它给出了两个分布之间的差异程度的量化，也就说相对熵代表的是这个两个分布的“距离”。两个概率密度函数  $p(x)$ 和  $q(x)$ 之间的相对熵定义为
$$
D(p \| q)=\sum_{x \in X} p(x) \log \frac{p(x)}{q(x)}\tag{12}
$$
从上式可以得到
$$
H(p,q)=H(p)+D(p \| q)\tag{13}
$$
证明如下
$$
\begin{aligned}H(p)+D(p \| q)&=-\sum_{x \in X}p(x)\log p(x) + \sum_{x \in X} p(x) \log \frac{p(x)}{q(x)} \\&=\sum_{x \in X}p(x)\left(\log \frac{p(x)}{q(x)}-\log p(x)\right) \\& = -\sum_{x \in X}p(x)\log q(x) = H(p,q)\end{aligned}\tag{14}
$$

## 参考文献

[1] [https://zh.wikipedia.org/wiki/%E5%85%8B%E5%8A%B3%E5%BE%B7%C2%B7%E9%A6%99%E5%86%9C](https://zh.wikipedia.org/wiki/克劳德·香农)

[2] https://zhuanlan.zhihu.com/p/36192699

[3] https://www.anmou.me/20180718-Entropy_and_Related_Concept/