【求证】当概率均等时信息熵最大，即当 $p(x_i)=\frac{1}{n}$ 时，信息熵取得最大值
$$
H=-\sum_{i=1}^{n} p(x_i) \log p(x_i) = \log (n)
$$
【证明】由柯西不等式得到
$$
\sum_{i=1}^{n}a_i \ge n\sqrt{a_1a_2 \ldots a_n}
$$
当 $a_1=a_2=\ldots=a_n$ 时，上式取等号，我们回到需要证明的式子，也就是说当 $p(x_i) \log p(x_i)$ 都相等时，$\sum_{i=1}^{n} p(x_i) \log p(x_i)$ 取的最小值，如果 $p(x_i) \log p(x_i)$ 相等，那只有一种情况就是 $p(x_i)=\frac{1}{n}$ ，所以这时候
$$
\max {H}=-\sum_{i=1}^{n} p(x_i) \log p(x_i) |_{p(x_i)=1/n}=-\sum_{i=1}^{n} \frac{1}{n} \log \frac{1}{n}=\log(n)
$$
